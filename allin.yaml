---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
# исходя из нагрузки ночью, поставим 2 реплики
  replicas: 2
  selector:
    matchLabels:
      app: web-app
# при обновлении будем заменять поды по одному
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      # разрешаем превысить обычное количество подов на 1,
      # чтобы количество рабочих было не меньше нужного
      # в соответствии с ночной нагрузкой и выставленным PodDisruptionBudget
      maxSurge: 1
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
        - name: web-app
          image: nginx:latest
          ports:
            - containerPort: 80
          # Сколько выделяем CPU и памяти
          resources:
            # 100 - минимум для работы
            requests:
              cpu: "100m"
              memory: "128Mi"
            # сколько нужно на старте?
            # ставлю побольше, и на всякий случай запас по памяти
            # если хотим тратить ресурсов поменьше - стоит узнать точнее
            limits:
              cpu: "1000m"
              memory: "256Mi"
          # Приложение грузится по 5-10 сек - проверять, загрузилось или нет, начнем через 10 секунд
          readinessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 3
          # периодически проверяем, живо ли приложение,
          # изначально дадим времени побольше, раз грузится долго
          livenessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 20
            periodSeconds: 10
            failureThreshold: 3
      # Размещаем поды my-app по разным зонам
      # Стараемся равномерно, чтобы повысить отказоустойчивость
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          # Если не может равномерно - пусть распределит как сможет,
          # чтобы подов хватало и вывезти нагрузку
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: web-app



# не очень поняла, при нагрузке растет потребление CPU или нет
# - если исходить только из времени суток и того, что днем нагрузка на порядок больше
# и с ней справляются 4 пода
# то можем поставить масштабирование по расписанию через CronJob
# утром - масштабируем до 4 реплик (если хотим запас и отказоустойчивость - до 5)
# к ночи - масштабируем до 2 реплик (справится и одна - но это если очень экономим,
# берем вторую прозапас, вдруг что с подом/нодой/зоной)
# но это не самое эффективное потребление ресурсов, т.к. не связано напрямую с нагрузкой
# - если есть разница по загрузке CPU,
# то можно сделать масштабирование от 2 до 4 (5) реплик через HorizontalPodAutoscaler
# здесь будет эффективнее по ресурсам
# - если снимаются метрики, можно попробовать по ним сделать масштабирование,
# исходя, например, из количества запросов - это позволит уменьшить затраты на ресурсы

---
# обеспечиваем минимальное число подов в работе - 2
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: web-app

---
apiVersion: v1
kind: Service
metadata:
  name: my-web-app-service
spec:
  selector:
    app: web-app
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-web-app-ingress
spec:
  ingressClassName: nginx
  rules:
    - host: my-app.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: my-web-app-service
                port:
                  number: 80

---
# в 23:00 на ночь уменьшаем число реплик до 2
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-down-my-web-app
spec:
  schedule: "0 23 * * *"
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 60
      template:
        spec:
          serviceAccountName: cronjob-sa
          containers:
            - name: scaler
              image: bitnami/kubectl
              command:
                - kubectl
                - scale
                - deployment
                - web-app
                - --replicas=2
          restartPolicy: OnFailure
---
# в 7:00 на день увеличиваем число реплик до 4
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-up-my-web-app
spec:
  schedule: "0 7 * * *"
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 60
      template:
        spec:
          serviceAccountName: cronjob-sa
          containers:
            - name: scaler
              image: bitnami/kubectl
              command:
                - kubectl
                - scale
                - deployment
                - web-app
                - --replicas=4
          restartPolicy: OnFailure
---
# сервис-аккаунт с правами для cronjob
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cronjob-sa
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cronjob-role
  namespace: default
rules:
  - apiGroups: ["apps"]
    resources: ["deployments", "deployments/scale"]
    verbs: ["get", "update", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cronjob-rolebinding
  namespace: default
subjects:
  - kind: ServiceAccount
    name: cronjob-sa
    namespace: default
roleRef:
  kind: Role
  name: cronjob-role
  apiGroup: rbac.authorization.k8s.io
---
